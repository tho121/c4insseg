{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate the same model configs, create model, and load model parameters from last run. Generate predictions on a new dataset, save annotations into json (get initial json from CVAT in COCO format)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tho121/c4project_final/venv/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "from d2go.runner import GeneralizedRCNNRunner\n",
    "from d2go.model_zoo import model_zoo\n",
    "\n",
    "def prepare_for_launch():\n",
    "    runner = GeneralizedRCNNRunner()\n",
    "    cfg = runner.get_default_cfg()\n",
    "    cfg.merge_from_file(\"c4_faster_rcnn_fbnetv3a_C4.yaml\")\n",
    "    cfg.MODEL_EMA.ENABLED = False\n",
    "    cfg.DATASETS.TRAIN = (\"c4_train\",)\n",
    "    cfg.DATASETS.TEST = (\"c4_val\",)\n",
    "    cfg.DATALOADER.NUM_WORKERS = 2\n",
    "    cfg.MODEL.WEIGHTS = None#model_zoo.get_checkpoint_url(\"faster_rcnn_fbnetv3a_C4.yaml\")  # Let training initialize from model zoo\n",
    "    cfg.MODEL.DEVICE = \"cpu\" if ('CI' in os.environ) else \"cuda\"\n",
    "    cfg.SOLVER.IMS_PER_BATCH = 8\n",
    "    cfg.SOLVER.BASE_LR = 0.0005  # pick a good LR\n",
    "    cfg.SOLVER.MAX_ITER = 50000    # 600 iterations seems good enough for this toy dataset; you will need to train longer for a practical dataset\n",
    "    cfg.SOLVER.STEPS = []        # do not decay learning rate\n",
    "    cfg.SOLVER.OPTIMIZER = 'adamw_mt'\n",
    "    cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 128   # faster, and good enough for this toy dataset (default: 512)\n",
    "    cfg.MODEL.ROI_HEADS.NUM_CLASSES = 4  # only has one class (ballon). (see https://detectron2.readthedocs.io/tutorials/datasets.html#update-the-config-for-new-datasets)\n",
    "    # NOTE: this config means the number of classes, but a few popular unofficial tutorials incorrect uses num_classes+1 here.\n",
    "    os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
    "    return cfg, runner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:d2go.modeling.backbone.fbnet_v2:Build FBNet using unified arch_def:\n",
      "trunk\n",
      "- {'block_op': 'conv_k3', 'block_cfg': {'out_channels': 16, 'stride': 2}, 'stage_idx': 0, 'block_idx': 0}\n",
      "- {'block_op': 'ir_k3', 'block_cfg': {'out_channels': 16, 'stride': 1, 'expansion': 1, 'less_se_channels': False}, 'stage_idx': 0, 'block_idx': 1}\n",
      "- {'block_op': 'ir_k5', 'block_cfg': {'out_channels': 32, 'stride': 2, 'expansion': 4, 'less_se_channels': False}, 'stage_idx': 1, 'block_idx': 0}\n",
      "- {'block_op': 'ir_k5', 'block_cfg': {'out_channels': 32, 'stride': 1, 'expansion': 2, 'less_se_channels': False}, 'stage_idx': 1, 'block_idx': 1}\n",
      "- {'block_op': 'ir_k5', 'block_cfg': {'out_channels': 40, 'stride': 2, 'expansion': 4, 'less_se_channels': False}, 'stage_idx': 2, 'block_idx': 0}\n",
      "- {'block_op': 'ir_k3', 'block_cfg': {'out_channels': 40, 'stride': 1, 'expansion': 3, 'less_se_channels': False}, 'stage_idx': 2, 'block_idx': 1}\n",
      "- {'block_op': 'ir_k3', 'block_cfg': {'out_channels': 40, 'stride': 1, 'expansion': 3, 'less_se_channels': False}, 'stage_idx': 2, 'block_idx': 2}\n",
      "- {'block_op': 'ir_k3', 'block_cfg': {'out_channels': 40, 'stride': 1, 'expansion': 3, 'less_se_channels': False}, 'stage_idx': 2, 'block_idx': 3}\n",
      "- {'block_op': 'ir_k5', 'block_cfg': {'out_channels': 72, 'stride': 2, 'expansion': 4, 'less_se_channels': False}, 'stage_idx': 3, 'block_idx': 0}\n",
      "- {'block_op': 'ir_k3', 'block_cfg': {'out_channels': 72, 'stride': 1, 'expansion': 3, 'less_se_channels': False}, 'stage_idx': 3, 'block_idx': 1}\n",
      "- {'block_op': 'ir_k3', 'block_cfg': {'out_channels': 72, 'stride': 1, 'expansion': 3, 'less_se_channels': False}, 'stage_idx': 3, 'block_idx': 2}\n",
      "- {'block_op': 'ir_k3', 'block_cfg': {'out_channels': 72, 'stride': 1, 'expansion': 3, 'less_se_channels': False}, 'stage_idx': 3, 'block_idx': 3}\n",
      "- {'block_op': 'ir_k5', 'block_cfg': {'out_channels': 112, 'stride': 1, 'expansion': 4, 'less_se_channels': False}, 'stage_idx': 3, 'block_idx': 4}\n",
      "- {'block_op': 'ir_k5', 'block_cfg': {'out_channels': 112, 'stride': 1, 'expansion': 4, 'less_se_channels': False}, 'stage_idx': 3, 'block_idx': 5}\n",
      "- {'block_op': 'ir_k5', 'block_cfg': {'out_channels': 112, 'stride': 1, 'expansion': 4, 'less_se_channels': False}, 'stage_idx': 3, 'block_idx': 6}\n",
      "- {'block_op': 'ir_k5', 'block_cfg': {'out_channels': 112, 'stride': 1, 'expansion': 4, 'less_se_channels': False}, 'stage_idx': 3, 'block_idx': 7}\n",
      "WARNING:mobile_cv.arch.utils.helper:Arguments ['width_divisor', 'dw_skip_bnrelu', 'zero_last_bn_gamma'] skipped for op Conv2d\n",
      "INFO:d2go.modeling.backbone.fbnet_v2:Build FBNet using unified arch_def:\n",
      "rpn\n",
      "- {'block_op': 'ir_k5', 'block_cfg': {'out_channels': 112, 'stride': 1, 'expansion': 4, 'less_se_channels': False}, 'stage_idx': 0, 'block_idx': 0}\n",
      "- {'block_op': 'ir_k5', 'block_cfg': {'out_channels': 112, 'stride': 1, 'expansion': 4, 'less_se_channels': False}, 'stage_idx': 0, 'block_idx': 1}\n",
      "- {'block_op': 'ir_k5', 'block_cfg': {'out_channels': 112, 'stride': 1, 'expansion': 4, 'less_se_channels': False}, 'stage_idx': 0, 'block_idx': 2}\n",
      "INFO:d2go.modeling.backbone.fbnet_v2:Build FBNet using unified arch_def:\n",
      "bbox\n",
      "- {'block_op': 'ir_k3', 'block_cfg': {'out_channels': 128, 'stride': 2, 'expansion': 4}, 'stage_idx': 0, 'block_idx': 0}\n",
      "- {'block_op': 'ir_k3', 'block_cfg': {'out_channels': 128, 'stride': 1, 'expansion': 6}, 'stage_idx': 0, 'block_idx': 1}\n",
      "- {'block_op': 'ir_k3', 'block_cfg': {'out_channels': 128, 'stride': 1, 'expansion': 6}, 'stage_idx': 0, 'block_idx': 2}\n",
      "- {'block_op': 'ir_k3', 'block_cfg': {'out_channels': 160, 'stride': 1, 'expansion': 6}, 'stage_idx': 0, 'block_idx': 3}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg, runner = prepare_for_launch()\n",
    "model = runner.build_model(cfg)\n",
    "model.load_state_dict(torch.load(\"output/model_final.pth\")['model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class C4Predictor:\n",
    "    def __init__(self, model, min_size_test=384, max_size_test=510, input_format=\"RGB\"):\n",
    "        self.model = model\n",
    "        self.model.eval()\n",
    "\n",
    "        #self.aug = []#T.ResizeShortestEdge([min_size_test, min_size_test], max_size_test)\n",
    "\n",
    "        self.input_format = input_format\n",
    "\n",
    "    def __call__(self, original_image):\n",
    "\n",
    "        inputs = self.convert_input(original_image)\n",
    "        predictions = self.model([inputs])[0]\n",
    "        return predictions\n",
    "\n",
    "    def convert_input(self, original_image):\n",
    "        with torch.no_grad():  # https://github.com/sphinx-doc/sphinx/issues/4258\n",
    "            # Apply pre-processing to image.\n",
    "            if self.input_format == \"RGB\":\n",
    "                # whether the model expects BGR inputs or RGB\n",
    "                original_image = original_image[:, :, ::-1]\n",
    "            height, width = original_image.shape[:2]\n",
    "            #image = self.aug.get_transform(original_image).apply_image(original_image)\n",
    "            image = torch.as_tensor(original_image.astype(\"float32\").transpose(2, 0, 1))\n",
    "\n",
    "            inputs = {\"image\": image, \"height\": height, \"width\": width}\n",
    "\n",
    "            return inputs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get initial COCO annotations from CVAT (should have an empty 'annotations' field). Rename file to labels.json or change path in following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "f = open(\"datasets/set2/labels.json\")\n",
    "\n",
    "labels_json = json.load(f)\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "import json\n",
    "\n",
    "DATASET_DIR = \"datasets/set2\"\n",
    "LABEL_NAME = \"labels_set2\"\n",
    "\n",
    "f = open(\"datasets/set2/labels.json\")\n",
    "labels_json = json.load(f)\n",
    "f.close()\n",
    "\n",
    "predictor = C4Predictor(model, min_size_test=384, max_size_test=510)\n",
    "\n",
    "img_file_index = {}\n",
    "\n",
    "for anno in labels_json['images']:\n",
    "    img_file_index[anno['file_name']] = anno['id']\n",
    "\n",
    "count = 1\n",
    "for img_file in os.listdir(os.path.join(DATASET_DIR, \"clean\")):\n",
    "    img = Image.open(os.path.join(DATASET_DIR, \"clean\", img_file))\n",
    "    outputs = predictor(np.array(img))\n",
    "\n",
    "    img_id = img_file_index[img_file]\n",
    "\n",
    "    cls_lbls = outputs[\"instances\"].pred_classes.cpu().detach().numpy()\n",
    "    bbox = outputs[\"instances\"].pred_boxes\n",
    "\n",
    "    for i, bbox in enumerate(bbox):\n",
    "        bbox = bbox.cpu().detach().numpy().tolist()\n",
    "        w = bbox[3] - bbox[1]\n",
    "        h = bbox[2] - bbox[0]\n",
    "        area = w * h\n",
    "        bbox = [bbox[0], bbox[1], h, w]\n",
    "\n",
    "        anno_dict = {}\n",
    "        anno_dict[\"id\"] = count\n",
    "        anno_dict[\"image_id\"] = img_id\n",
    "        anno_dict[\"category_id\"] = int(cls_lbls[i]) + 1 #offset required\n",
    "        anno_dict[\"segmentation\"] = []\n",
    "        anno_dict[\"area\"] = area\n",
    "        anno_dict[\"bbox\"] = bbox\n",
    "        anno_dict[\"iscrowd\"] = 0\n",
    "        anno_dict[\"attributes\"] = {\"occluded\": False, \"rotation\": 0.0}\n",
    "\n",
    "        count += 1\n",
    "\n",
    "        labels_json['annotations'].append(anno_dict)\n",
    "\n",
    "\n",
    "json_object = json.dumps(labels_json, indent=4)\n",
    "\n",
    "# Writing to sample.json\n",
    "with open(LABEL_NAME + \".json\", \"w\") as outfile:\n",
    "    outfile.write(json_object)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "73d4c43128b3e98d773b144eae37bb73ef7dd2477516c54af042135a09cbb003"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
